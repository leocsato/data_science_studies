{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Existe uma técnica chamada Random Forest (em inglês), onde multíplas árvores de decisão são construídas e elas \"votam\" para predizer a classificação final de inputs.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As Random Forests são de fato um exemplo de bagging (Bootstrap Aggregating). O bagging é uma técnica de ensemble em que várias instâncias de um mesmo modelo são treinadas em diferentes subconjuntos aleatórios do conjunto de dados original, e as previsões são combinadas por votação (para classificação) ou média (para regressão).\n",
    "As Random Forests aplicam essa ideia ao usar uma coleção de árvores de decisão, treinadas em diferentes subconjuntos dos dados, e agregam suas previsões para melhorar a precisão e a robustez do modelo.\n",
    "\n",
    "No contexto de  Random Forests, o Bootstrap é utilizado para criar múltiplos conjuntos de dados de treinamento a partir do conjunto de dados original. Cada conjunto de dados de treinamento é gerado amostrando aleatoriamente observações do conjunto de dados original com substituição. Esses conjuntos de dados são então usados para treinar árvores de decisão individuais dentro da floresta.\n",
    "O processo específico é o seguinte:\n",
    "Amostragem com Substituição (Bootstrap): A partir do conjunto de dados original, várias amostras são criadas, cada uma contendo o mesmo número de observações, mas permitindo que algumas observações se repitam e outras não sejam incluídas.\n",
    "Treinamento de Árvores de Decisão: Para cada amostra bootstrap, uma árvore de decisão é treinada usando o conjunto de dados correspondente. Cada árvore é treinada de forma independente e pode usar diferentes subconjuntos de observações e atributos devido à natureza aleatória do processo de bootstrap e da seleção de atributos no Random Forest.\n",
    "Agregação de Previsões: Uma vez que todas as árvores foram treinadas, as previsões de cada árvore são combinadas para produzir uma previsão final. No caso de classificação, as previsões podem ser agregadas por votação da classe mais comum entre as árvores. Para regressão, as previsões podem ser agregadas por média.\n",
    "Ao utilizar o Bootstrap para criar conjuntos de dados de treinamento, a Random Forest consegue introduzir diversidade entre as árvores de decisão individuais, o que ajuda a reduzir o overfitting e a aumentar a precisão do modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T05:56:17.975595Z",
     "iopub.status.busy": "2024-04-26T05:56:17.975046Z",
     "iopub.status.idle": "2024-04-26T05:56:18.790427Z",
     "shell.execute_reply": "2024-04-26T05:56:18.789075Z",
     "shell.execute_reply.started": "2024-04-26T05:56:17.975551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14205973 0.76664038 0.0282433  0.06305659]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Esse algoritmo:\n",
    "- É capaz de lidar tanto com problemas de classificação quanto de regressão.\n",
    "- É um ensemble de diversos classificadores/regressores.\n",
    "- Introduz aleatoriedade tanto no subconjunto de dados quanto no subconjunto de features que cada classificador/regressor irá utilizar.\n",
    "- É baseado em Árvores de Decisão, porém é mais robusto contra overfitting.\n",
    "- Toda árvore na \"floresta\" vai votar no resultado, e a predição final é determinada pela maioria. \n",
    "- Usa bagging, ou seja, seleciona amostras aleatórias dos dados (com substituição) e treina modelos independentes.\n",
    "- Além disso, utilizada de aleatoriedade de features. Em resumo, gera um subconjunto aleatório das features para as árvores. Dessa forma, busca garantir baixa correlação entre as árvores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
